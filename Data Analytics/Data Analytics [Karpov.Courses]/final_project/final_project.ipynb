{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "# импорт библиотек\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import scipy.stats as ss\n",
    "# import pingouin as pg\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/karimalibekov/Desktop/my_python/jupyter-k-alibekov/shared/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# reading dataframes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# чтение датафреймов\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m reg_data  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/karimalibekov/Desktop/my_python/jupyter-k-alibekov/shared/\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      5\u001b[0m                         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m auth_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/HC_Volume_18315164/home-jupyter/jupyter-k-alibekov/shared/problem1-auth_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      7\u001b[0m                         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m ab_data   \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/HC_Volume_18315164/home-jupyter/jupyter-k-alibekov/shared/problem2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m                         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/karimalibekov/Desktop/my_python/jupyter-k-alibekov/shared/'"
     ]
    }
   ],
   "source": [
    "# reading dataframes\n",
    "# чтение датафреймов\n",
    "\n",
    "reg_data  = pd.read_csv('/Users/karimalibekov/Desktop/my_python/jupyter-k-alibekov/shared/', \n",
    "                        sep=';')\n",
    "auth_data = pd.read_csv('/mnt/HC_Volume_18315164/home-jupyter/jupyter-k-alibekov/shared/problem1-auth_data.csv', \n",
    "                        sep=';')\n",
    "ab_data   = pd.read_csv('/mnt/HC_Volume_18315164/home-jupyter/jupyter-k-alibekov/shared/problem2.csv', \n",
    "                        sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview & Preprocessing // Обзор и подготовка датафреймов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF №1 – reg_data // Датафрейм №1 – reg-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "# просмотр\n",
    "\n",
    "reg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dtypes, non-null count and shape \n",
    "# смотрим на типы данных, пропущенные значения и размер датафрейма\n",
    "\n",
    "reg_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "# проверка упущенных значений\n",
    "\n",
    "missing_values = reg_data.isnull().sum()\n",
    "\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing timestamp into datetime\n",
    "# смена формата timestamp на datetime\n",
    "\n",
    "reg_data['reg_ts'] = pd.to_datetime(reg_data['reg_ts'], unit='s')\n",
    "reg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "# переименование колонок\n",
    "\n",
    "reg_data = reg_data.rename(columns={'reg_ts':'reg_date', 'uid':'user_id'})\n",
    "reg_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF №2 – auth_data // Датафрейм №2 – auth_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "# просмотр\n",
    "\n",
    "auth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dtypes, non-null count and shape \n",
    "# смотрим на типы данных, пропущенные значения и размер датафрейма\n",
    "\n",
    "auth_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "# проверка упущенных значений\n",
    "\n",
    "missing_values = auth_data.isnull().sum()\n",
    "\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing timestamp into datetime\n",
    "# смена формата timestamp на datetime\n",
    "\n",
    "auth_data['auth_ts'] = pd.to_datetime(auth_data['auth_ts'], unit='s')\n",
    "auth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "# переименование колонок\n",
    "\n",
    "auth_data = auth_data.rename(columns={'auth_ts':'auth_date', 'uid':'user_id'})\n",
    "auth_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF №3 – ab_data // Датафрейм №3 – ab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "# просмотр\n",
    "\n",
    "ab_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dtypes, non-null count and shape \n",
    "# смотрим на типы данных, пропущенные значения и размер датафрейма\n",
    "\n",
    "ab_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "# проверка упущенных значений\n",
    "\n",
    "missing_values = ab_data.isnull().sum()\n",
    "\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 // Задание 1\n",
    "\n",
    "## [ENG]\n",
    "\n",
    "### Retention is one of the most important metrics in a company. Your task is to write a function that will count the retention of players (by days from the date of player registration) \n",
    "\n",
    "## [RUS]\n",
    "\n",
    "### Retention – один из самых важных показателей в компании. Ваша задача – написать функцию, которая будет считать retention игроков (по дням от даты регистрации игрока)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes\n",
    "# слияние датафреймов\n",
    "\n",
    "df = auth_data.merge(reg_data, on='user_id', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dtypes and shape \n",
    "# смотрим на типы данных и размер датафрейма\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "# проверка упущенных значений\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reordering columns\n",
    "# перестановка колонок\n",
    "\n",
    "df = df[['user_id', 'reg_date', 'auth_date']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping minutes & seconds info from date\n",
    "# удаляем информацию про минуты и секунды из даты\n",
    "\n",
    "df['reg_date'] = df['reg_date'].apply(lambda x: datetime(x.year, x.month, x.day))\n",
    "df['auth_date'] = df['auth_date'].apply(lambda x: datetime(x.year, x.month, x.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a year registration column \n",
    "# создание колонки с годом регистрации\n",
    "\n",
    "df['reg_year'] = df.reg_date.dt.year\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of number of registrations\n",
    "# визуализация количества регистраций\n",
    "\n",
    "fig = px.bar(df.groupby('reg_year', as_index=False).agg({'user_id':'count'}), \n",
    "             x='reg_year', y='user_id', labels={'reg_year':'year', 'user_id':'number of registrations'}, \n",
    "             title='Number of registrations over the years 1998–2020')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sample of data, since the size of dataframe is big (for instance: data from 2020-01-01)\n",
    "# берем сэмпл данных (за 2019 год, так как, так как размер датафрейма велик (например данные с 2020-01-01)\n",
    "\n",
    "sample = df.query(\"reg_date >= '2020-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Month Cohorts // Когорты по месяцам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We assume that registration date == first authentification date == first interaction with game\n",
    "\n",
    "### Мы предполагаем, что дата регистрации == дата первой аутентификации == первое взаимодействие с игрой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a cohort month with first day of the month\n",
    "# создание когорт по первому дню месяца\n",
    "\n",
    "sample['cohort_month'] = sample['reg_date'].apply(lambda x: dt.datetime(x.year, x.month, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping reg_date, since it has the same meaning as cohort month due to assumption above\n",
    "# отбрасываем reg_date, так как она имеет то же значение, что и cohort month, в связи с предположением выше\n",
    "\n",
    "sample = sample[['user_id', 'auth_date', 'cohort_month']]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a date element function to get a series for subtraction\n",
    "# создание функции вычленения элемента даты в виде серии для последующего вычитания\n",
    "\n",
    "def get_date_elements(df, column):\n",
    "    day = df[column].dt.day\n",
    "    month = df[column].dt.month\n",
    "    year = df[column].dt.year\n",
    "    return day, month, year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date elements for our cohort and invoice columns\n",
    "# применение функции для колонок Cohort Month и Invoice Month\n",
    "\n",
    "_,auth_month,auth_year =  get_date_elements(sample, 'auth_date')\n",
    "_,cohort_month,cohort_year =  get_date_elements(sample, 'cohort_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the series \n",
    "# проверка полученной серии\n",
    "\n",
    "cohort_year[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cohort index (in number of months, +1 is added so that there is no month zero)\n",
    "# создание индекса когорты (в количестве месяцев, +1 добавляется, чтобы не было нулевого месяца)\n",
    "\n",
    "year_diff = auth_year - cohort_year\n",
    "month_diff = auth_month - cohort_month\n",
    "sample['cohort_index'] = year_diff*12+month_diff+1\n",
    "sample.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the user id using grouping by Cohort Month  and Cohort Index \n",
    "# считаем user id, используя группировку по месяцам когорты и индексу когорты\n",
    "\n",
    "cohort_data = sample.groupby(['cohort_month','cohort_index'])['user_id'].apply(pd.Series.nunique).reset_index()\n",
    "cohort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pivot table \n",
    "# создание сводной таблицы\n",
    "\n",
    "cohort_table = cohort_data.pivot(index='cohort_month', columns=['cohort_index'],values='user_id')\n",
    "cohort_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index format\n",
    "# изменение формата индекса \n",
    "\n",
    "cohort_table.index = cohort_table.index.strftime('%B %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of our results in heatmap\n",
    "# визуализация результата в виде тепловой карты\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(cohort_table, annot=True, cmap='magma');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort table as a percentage of the first month\n",
    "# когортная таблица в виде процента от первого месяца\n",
    "\n",
    "new_cohort_table = cohort_table.divide(cohort_table.iloc[:,0],axis=0)\n",
    "new_cohort_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap visualization in %\n",
    "# тепловая карта с %\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(new_cohort_table, annot=True, cmap='magma', fmt='.0%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day Cohorts // Когорты по дням"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating another sample for day cohorts\n",
    "# создаем еще один сэмпл данных для когорт по дням\n",
    "\n",
    "sample_2 = df.query(\"reg_date >= '2020-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing needed columns\n",
    "# выбираем необходимые колонки\n",
    "\n",
    "sample_2 = sample_2[['user_id', 'reg_date', 'auth_date']]\n",
    "sample_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming for better understanding\n",
    "# переименование для лучшего понимания\n",
    "\n",
    "sample_2 = sample_2.rename(columns={'reg_date':'cohort_day'})\n",
    "sample_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date elements for our cohort and invoice columns\n",
    "# применение функции для колонок Cohort Month и Invoice Month\n",
    "\n",
    "auth_day,auth_month,_ =  get_date_elements(sample_2, 'auth_date')\n",
    "cohort_day,cohort_month,_ =  get_date_elements(sample_2, 'cohort_day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the series \n",
    "# проверка полученной серии\n",
    "\n",
    "cohort_day[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cohort index (in number of days, +1 is added so that there is no day zero)\n",
    "# создание индекса когорты (в количестве дней, +1 добавляется, чтобы не было нулевого дня)\n",
    "\n",
    "day_diff = auth_day - cohort_day\n",
    "month_diff = auth_month - cohort_month\n",
    "sample_2['cohort_index'] = day_diff+1+month_diff*30\n",
    "sample_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the user id using grouping by Cohort Month  and Cohort Index \n",
    "# считаем user id, используя группировку по месяцам когорты и индексу когорты\n",
    "\n",
    "cohort_data_2 = sample_2.groupby(['cohort_day','cohort_index'])['user_id'].apply(pd.Series.nunique).reset_index()\n",
    "cohort_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pivot table \n",
    "# создание сводной таблицы\n",
    "\n",
    "cohort_table_2 = cohort_data_2.pivot(index='cohort_day', columns=['cohort_index'],values='user_id')\n",
    "cohort_table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index format\n",
    "# изменение формата индекса \n",
    "\n",
    "cohort_table_2.index = cohort_table_2.index.strftime('%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort table as a percentage of the first day\n",
    "# когортная таблица в виде процента от первого дня\n",
    "\n",
    "new_cohort_table_2 = cohort_table_2.divide(cohort_table_2.iloc[:,0],axis=0)\n",
    "new_cohort_table_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting Function // Итоговая функция \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe sample to put into function\n",
    "# создаем еще один сэмпл датафрейма для подстановки в аргумент функции\n",
    "\n",
    "sample_3 = df.query(\"reg_date >= '2020-09-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cohort table\n",
    "# функция, строящая когортную таблицу\n",
    "\n",
    "def retention_analysis(df, user_id_column, min_date_column, action_date_column):\n",
    "    \n",
    "    data = df\n",
    "    \n",
    "    auth_day,auth_month,_ =  get_date_elements(data, action_date_column)\n",
    "    cohort_day,cohort_month,_ =  get_date_elements(data, min_date_column)\n",
    "    \n",
    "    day_diff = auth_day - cohort_day\n",
    "    month_diff = auth_month - cohort_month\n",
    "    data['cohort_index'] = day_diff+1+month_diff*30\n",
    "    \n",
    "    cohort_data_ = data.groupby([min_date_column, 'cohort_index'])[user_id_column].apply(pd.Series.nunique).reset_index()\n",
    "\n",
    "    cohort_table_ = cohort_data_.pivot(index=min_date_column, columns=['cohort_index'], values= user_id_column)\n",
    "    \n",
    "    cohort_table_.index = cohort_table_.index.strftime('%d')\n",
    "    \n",
    "    new_cohort_table_ = cohort_table_.divide(cohort_table_.iloc[:,0],axis=0)\n",
    "    \n",
    "    return new_cohort_table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function call\n",
    "# вызов функции\n",
    "\n",
    "results = retention_analysis(sample_3, 'user_id', 'reg_date', 'auth_date')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap visualization in %\n",
    "# тепловая карта с %\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(results, annot=True, cmap='Blues', fmt='.0%', annot_kws={'fontsize':13});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 // Задание 2\n",
    "\n",
    "## ENG\n",
    "\n",
    "### There are the results of an A/B test in which two groups of users were offered different sets of promotional offers. It is known that ARPU in the test group is 5% higher than in the control group. At the same time in the control group 1928 players out of 202103 turned out to be paying players, and in the test group - 1805 out of 202667.\n",
    "\n",
    "### Which set of offers can be considered the best? What metrics should be analyzed to make the right decision and how?\n",
    "\n",
    "## RUS\n",
    "\n",
    "### Имеются результаты A/B теста, в котором двум группам пользователей предлагались различные наборы акционных предложений. Известно, что ARPU в тестовой группе выше на 5%, чем в контрольной. При этом в контрольной группе 1928 игроков из 202103 оказались платящими, а в тестовой – 1805 из 202667.\n",
    "\n",
    "### Какой набор предложений можно считать лучшим? Какие метрики стоит проанализировать для принятия правильного решения и как?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan // План: \n",
    "\n",
    "#### 1. All users with anomalies // Все пользователи с аномалиями \n",
    "\n",
    "    – descriptive statistics // описательная статистика\n",
    "\n",
    "    – visualizations // визуализации\n",
    "\n",
    "    – normality test // тест на нормальность\n",
    "\n",
    "    – t-test // t-тест\n",
    "\n",
    "    \n",
    "#### 2. Paying users with anomalies // Платящие пользователи с аномалиями\n",
    "\n",
    "    – descriptive statistics // описательная статистика\n",
    "\n",
    "    – visualizations // визуализации\n",
    "\n",
    "    – normality test // тест на нормальность\n",
    "\n",
    "    – t-test // t-тест\n",
    "    \n",
    "#### 3. Paying users without anomalies // Платящие пользователи без аномалий\n",
    "\n",
    "    – descriptive statistics // описательная статистика\n",
    "\n",
    "    – visualizations // визуализации\n",
    "\n",
    "    – normality test // тест на нормальность\n",
    "\n",
    "    – t-test // t-тест\n",
    "    \n",
    "#### 4. Log version of paying users with anomalies // Платящие пользователи с аномалиями + логарифмическое преобразование\n",
    "\n",
    "    – descriptive statistics // описательная статистика\n",
    "\n",
    "    – visualizations // визуализации\n",
    "\n",
    "    – normality test // тест на нормальность\n",
    "\n",
    "    – t-test // t-тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. All users with anomalies (paying + not paying + anomalies) // Все пользователи с аномалиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of dataframe\n",
    "# размер датафрейма\n",
    "\n",
    "ab_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique users count\n",
    "# количество уникальных пользователей\n",
    "\n",
    "ab_data.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of users in each group\n",
    "# количество пользователей в каждой группе\n",
    "\n",
    "ab_data.testgroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of users in each group in %\n",
    "# количество пользователей в каждой группе в %\n",
    "\n",
    "ab_data.testgroup.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revenue sum, mean, median, max, min for each group\n",
    "# сумма, медиана, среднее, максимальное и минимальное значение выручки для каждой группы \n",
    "\n",
    "ab_data.groupby('testgroup', as_index=False).agg({'user_id':'nunique', 'revenue':['sum', 'mean', 'median',\n",
    "                                                                                  'max', 'min']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that mean is slightly bigger in group b, median is 0, since most of our users do not pay, also max value of 37433 in group a is very high, we must check if it is anomaly or not**\n",
    " \n",
    "**Мы видим, что среднее значение немного больше в группе b, медиана равна 0, так как большинство наших пользователей не платят, также максимальное значение 37433 в группе a очень велико, мы должны проверить, является ли это аномалией или нет**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of distribution of all a/b test participants (not paying + paying)\n",
    "# визуализация распределения всех участников а/б тестирования (не платящие + платящие)\n",
    "\n",
    "fig = px.histogram(ab_data, x=\"revenue\", color=\"testgroup\", color_discrete_sequence=[\"blue\", \"red\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's hard to understand something from visualization above since we have big differences in values, so we need another way of displaying the data due to some filtering of origin dataframe**\n",
    "\n",
    "**Из приведенной визуализации сложно что-то понять, так как у нас большие различия в значениях, поэтому нам нужен другой способ отображения данных, связанный с некоторой фильтрацией исходного датафрейма**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way of above visualization\n",
    "# альтернативный способ приведенной выше визуализации\n",
    "\n",
    "sns.kdeplot(ab_data.query(\"testgroup == 'a'\").revenue, color='red', fill=True)\n",
    "sns.kdeplot(ab_data.query(\"testgroup == 'b'\").revenue, color='blue', fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's still check distributions for both groups for normality, these functions test the null hypothesis that a sample comes from a normal distribution.**\n",
    "\n",
    "**Давайте все-таки проверим распределения для обеих групп на нормальность, эти функции проверяют нулевую гипотезу о том, что выборка имеет нормальное распределение.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(ab_data.query(\"testgroup == 'a'\").revenue))\n",
    "print(ss.normaltest(ab_data.query(\"testgroup == 'a'\").revenue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(ab_data.query(\"testgroup == 'b'\").revenue))\n",
    "print(ss.normaltest(ab_data.query(\"testgroup == 'b'\").revenue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In both groups we reject null hypotheses that data is distributed normally**\n",
    "\n",
    "**В обеих группах мы отвергаем нулевые гипотезы о том, что данные распределены нормально**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.ttest_ind(ab_data.query(\"testgroup == 'a'\").revenue, ab_data.query(\"testgroup == 'b'\").revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion // Выводы\n",
    "\n",
    "    – We see that null hypothesis is not rejected in ttest, since p-value > 0.05\n",
    "    – We need to compare groups data with filters and also to work with anomalies\n",
    "    – So far we cannot say that a/b test is successful \n",
    "\n",
    "    – Мы видим, что нулевая гипотеза в ttest не отвергается, так как p-значение > 0,05\n",
    "    – Нам необходимо сравнивать данные групп с помощью фильтров, а также работать с аномалиями\n",
    "    – Пока мы не можем сказать, что a/b-тест является успешным "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Paying users with anomalies (only paying + anomalies) // Платящие пользователи с аномалиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revenue sum, mean, median, max, min for each group among paying users\n",
    "# сумма, медиана, среднее, максимальное и минимальное значение выручки для каждой группы среди платящих пользователей\n",
    "\n",
    "ab_data.query(\"revenue > 0\").groupby('testgroup', as_index=False).agg({'user_id':'nunique', \n",
    "                                                                       'revenue':['sum', 'mean', 'median', \n",
    "                                                                                  'max', 'min']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median is not 0 anymore, since we cut all non-paying users, let's see if it helps to provide more statistical significant results**\n",
    "\n",
    "**Медиана уже не равняется 0, так как мы отсекли всех неплатящих пользователей, посмотрим, поможет ли это получить более статистически значимые результаты**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of all paying users\n",
    "# гистограмма всех платящих пользователей\n",
    "\n",
    "fig = px.histogram(ab_data.query(\"revenue > 0\"), x=\"revenue\", color=\"testgroup\", \n",
    "                   color_discrete_sequence=[\"red\", \"blue\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we can obviously see that we have ultra-high values in group A, which we can call anomalous, speaking from business point of view this segment is people who make biggest in-game purchases**\n",
    "\n",
    "**Здесь мы видим, что в группе А мы имеем сверхвысокие значения, которые можно назвать аномальными, с точки зрения бизнеса этот сегмент - люди, совершающие самые крупные внутриигровые покупки**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of distribution of paying users by group with revenue mean\n",
    "# визуализация распределения платящих пользователей по группам с отображением среднего значения выручки\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.kdeplot(ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue, color='red', fill=True)\n",
    "sns.kdeplot(ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue, color='blue', fill=True)\n",
    "\n",
    "plt.axvline(2663.998444, c='red', linestyle='--', label = 'mean (revenue > 0) for group a') \n",
    "plt.axvline(3003.658172, c='blue', linestyle='--', label = 'mean (revenue > 0) for group b')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this plot we can see that in spite of group A has anomalous values, mean for group B is still more**\n",
    "\n",
    "**На этом графике видно, что, несмотря на то, что группа А имеет аномальные значения, среднее значение для группы В все равно больше**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of distribution of paying users by group with revenue median\n",
    "# визуализация распределения платящих пользователей по группам с отображением медианного значения выручки\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.kdeplot(ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue, color='red', fill=True)\n",
    "sns.kdeplot(ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue, color='blue', fill=True)\n",
    "\n",
    "plt.axvline(311, c='red', linestyle='--', label = 'median (revenue > 0) for group a') \n",
    "plt.axvline(3022, c='blue', linestyle='--', label = 'median (revenue > 0) for group b')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median for group B is also bigger**\n",
    "\n",
    "**Медиана для группы B также больше**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot to see quantiles, anomalies among paying users\n",
    "# скрипичный график для отображения квантилей, поиска аномалий среди платящих пользователей\n",
    "\n",
    "fig = px.violin(ab_data.query(\"revenue > 0\"), y=ab_data.query(\"revenue > 0\").revenue, x=\"testgroup\", color=\"testgroup\", \n",
    "                color_discrete_sequence=[\"red\", \"blue\"], box=True, points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# share of anomalous paying users among all paying users\n",
    "# доля аномальных платящих пользователей среди всех платящих пользователей\n",
    "\n",
    "len(ab_data.query(\"revenue > 20000\")) / len(ab_data.query(\"revenue > 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Share of anomalous users is less than 5%, so we can drop it in future research**\n",
    "\n",
    "**Доля аномальных пользователей составляет менее 5%, поэтому мы можем отказаться от нее в будущих исследованиях**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue))\n",
    "print(ss.normaltest(ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue))\n",
    "print(ss.normaltest(ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In both groups we reject null hypotheses that data is distributed normally**\n",
    "\n",
    "**В обеих группах мы отвергаем нулевые гипотезы о том, что данные распределены нормально**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.ttest_ind((ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue), \n",
    "             ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion // Выводы\n",
    "\n",
    "    – We see that null hypothesis is not rejected in ttest, since p-value > 0.05\n",
    "    – We need to compare groups data without anomalies\n",
    "    – So far we cannot say that a/b test is successful \n",
    "\n",
    "    – Мы видим, что нулевая гипотеза в ttest не отвергается, так как p-значение > 0,05\n",
    "    – Нам необходимо сравнивать данные групп без аномалий\n",
    "    – Пока мы не можем сказать, что a/b-тест является успешным "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Paying users without anomalies (paying + without anomalies) // Платящие пользователи без аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data_2 = ab_data.query(\"0 < revenue < 20000\")\n",
    "ab_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule of revenue < 20000 helps us to inspect data without anomalous values**\n",
    "\n",
    "**Правило выручки < 20000 помогает нам проверять данные без аномальных значений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we see that max value of revenue for both groups is 4000**\n",
    "\n",
    "**Теперь мы видим, что максимальное значение выручки для обеих групп равно 4000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data_2.groupby('testgroup', as_index=False).agg({'user_id':'nunique', \n",
    "                                                    'revenue':['sum', 'mean', 'median', \n",
    "                                                                        'max', 'min']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.kdeplot(ab_data_2.query(\"testgroup == 'a'\").revenue, color='red', fill=True)\n",
    "sns.kdeplot(ab_data_2.query(\"testgroup == 'b'\").revenue, color='blue', fill=True)\n",
    "\n",
    "plt.axvline(302.458172, c='red', linestyle='--', label = 'mean (20000 > revenue > 0) for group a') \n",
    "plt.axvline(3003.658172, c='blue', linestyle='--', label = 'mean (20000 > revenue > 0 for group b')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.kdeplot(ab_data_2.query(\"testgroup == 'a'\").revenue, color='red', fill=True)\n",
    "sns.kdeplot(ab_data_2.query(\"testgroup == 'b'\").revenue, color='blue', fill=True)\n",
    "\n",
    "plt.axvline(305, c='red', linestyle='--', label = 'median (20000 > revenue > 0) for group a') \n",
    "plt.axvline(3022, c='blue', linestyle='--', label = 'median (20000 > revenue > 0 for group b')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now without anomalous values we see that group B has significantly higher values in mean and median values**\n",
    "\n",
    "**Теперь без аномальных значений мы видим, что группа В имеет значительно более высокие значения средних и медианных величин**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(ab_data_2, y=\"revenue\", x=\"testgroup\", color=\"testgroup\", color_discrete_sequence=[\"red\", \"blue\"], \n",
    "                box=True, points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Violin plot also shows us that we don't have 'tail' like before in group A**\n",
    "\n",
    "**Скрипичный график также показывает, что у нас нет \"хвоста\", как раньше в группе A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see distributions for both groups and make normality tests + ttest in the end**\n",
    "\n",
    "**Посмотрим распределения для обеих групп и проведем проверку на нормальность + ttest в конце**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ab_data_2.query(\"testgroup == 'a'\").revenue, color='red', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(ab_data_2.query(\"testgroup == 'a'\").revenue)) \n",
    "print(ss.normaltest(ab_data_2.query(\"testgroup == 'a'\").revenue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ab_data_2.query(\"testgroup == 'b'\").revenue, color='blue', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(ab_data_2.query(\"testgroup == 'b'\").revenue))\n",
    "print(ss.normaltest(ab_data_2.query(\"testgroup == 'b'\").revenue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.ttest_ind((ab_data_2.query(\"testgroup == 'a'\").revenue), (ab_data_2.query(\"testgroup == 'b'\").revenue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion // Выводы\n",
    "\n",
    "    – Our distributions are still not normal, but we have enough data to perform a test without loosing accuracy of p-value \n",
    "    – We see that null hypothesis is rejected in ttest, since p-value < 0.05\n",
    "    – We can say that a/b test is successful (statistically significant + test group shows better results)\n",
    "    – We also can compare paying users with anomalies with log-transformation\n",
    "    \n",
    "    - Наши распределения все еще не являются нормальными, но у нас достаточно данных для проведения теста без потери точности p-значения \n",
    "    - Мы видим, что нулевая гипотеза в ttest отвергается, поскольку p-value < 0,05.\n",
    "    - Можно сказать, что a/b-тест прошел успешно (статистически значимо + тестовая группа показала лучшие результаты)\n",
    "    – Мы также можем сравнить платящих пользователей с аномалиями с помощью логарифмического преобразования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Log version of paying users with anomalies (only paying + anomalies) // Платящие пользователи с аномалиями + логарифмическое преобразование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.query(\"revenue > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizations of distributions before log-transformation**\n",
    "\n",
    "**Визуализация распределений до лог-преобразования**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue, color='red', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue, color='blue', fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log-transformation**\n",
    "\n",
    "**Лог-преобразование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_a_log = np.log(ab_data.query(\"revenue > 0 & testgroup == 'a'\").revenue)\n",
    "revenue_a_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_b_log = np.log(ab_data.query(\"revenue > 0 & testgroup == 'b'\").revenue)\n",
    "revenue_b_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizations of distributions after log-transformation**\n",
    "\n",
    "**Визуализация распределений после лог-преобразования**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(revenue_a_log, color='red', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(revenue_b_log, color='blue', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('revenue_a_log mean is: ', revenue_a_log.mean())\n",
    "print('revenue_b_log mean is: ', revenue_b_log.mean())\n",
    "print('revenue_a_log median is: ', revenue_a_log.median())\n",
    "print('revenue_b_log median is: ', revenue_b_log.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After log-transformation group B is still leader in revenue mean and median values**\n",
    "\n",
    "**После лог-трансформации группа B по-прежнему лидирует по средним и медианным значениям выручки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(revenue_a_log))\n",
    "print(ss.normaltest(revenue_a_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.shapiro(revenue_b_log))\n",
    "print(ss.normaltest(revenue_b_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.ttest_ind(revenue_a_log, revenue_b_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion // Выводы\n",
    "\n",
    "    – Our distributions are still not normal, but we have enough data to perform a test without loosing accuracy of p-value \n",
    "    – We see that null hypothesis is rejected in ttest, since p-value < 0.05\n",
    "    – We can say that a/b test is successful (statistically significant + test group shows better results)\n",
    "    – Anomalies do not affect signigicance test with log-transformation of data\n",
    "    \n",
    "    - Наши распределения все еще не являются нормальными, но у нас достаточно данных для проведения теста без потери точности p-значения \n",
    "    - Мы видим, что нулевая гипотеза в ttest отвергается, поскольку p-value < 0,05.\n",
    "    - Можно сказать, что a/b-тест прошел успешно (статистически значимо + тестовая группа показала лучшие результаты)\n",
    "    - Аномалии не влияют на проверку значимости при логарифмическом преобразовании данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results // Итоги по заданию\n",
    "\n",
    "    - In any of the scenarios, group B shows better values compared to group A (paying / non-paying / with anomalies / without anomalies / log-transformation)\n",
    "    - Considering still that ARPU in the test group is 5% higher than in the control group, we can clearly conclude that the test group has better results\n",
    "    - The company should use the set of suggestions for the test group for all groups\n",
    "\n",
    "    – В любом из сценариев группа Б показывает лучшие значения по сравнению с группой А (платящие / не платящие / с аномалиями / без аномалий / лог-трансформация)\n",
    "    – Учитывая еще, что ARPU в тестовой группе выше на 5%, чем в контрольной, можно сделать однозначный вывод, что тестовая группа имеет лучшие результаты\n",
    "    – Компании стоит использовать набор предложений для тестовой группы для всех групп"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 // Задание 3\n",
    "\n",
    "## ENG\n",
    "\n",
    "### The Plants & Gardens game features themed events every month for a limited time. In them, players can get unique items for the garden and characters, additional coins or bonuses. The rewards require completing a series of levels in a certain amount of time. What metrics can be used to evaluate the results of the most recent past event?\n",
    "\n",
    "### Suppose in another event we have complicated the event mechanics so that every time the player fails to complete a level, the player will be rolled back several levels. Would the set of outcome evaluation metrics change? If so, how?\n",
    "\n",
    "## RUS\n",
    "\n",
    "### В игре Plants & Gardens каждый месяц проводятся тематические события, ограниченные по времени. В них игроки могут получить уникальные предметы для сада и персонажей, дополнительные монеты или бонусы. Для получения награды требуется пройти ряд уровней за определенное время. С помощью каких метрик можно оценить результаты последнего прошедшего события?\n",
    "\n",
    "### Предположим, в другом событии мы усложнили механику событий так, что при каждой неудачной попытке выполнения уровня игрок будет откатываться на несколько уровней назад. Изменится ли набор метрик оценки результата? Если да, то как?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "### Metrics for scenario 1 (each unsuccessful level attempt will NOT cause the player to roll back a few levels)\n",
    "\n",
    "> Average number of levels completed by the user per event\n",
    "\n",
    "> Average time to complete each level per event\n",
    "\n",
    "> Average time to complete all levels per event (for those who have completed all levels)\n",
    "\n",
    "> Churn rate for each level (which levels were particularly hard for users)\n",
    "\n",
    "> Average number of unique items a user received by different categories (garden, characters, extra coins, bonuses)\n",
    "\n",
    "> The number of new unique users during the event\n",
    "\n",
    "> Rate of users returning to the app during the themed event\n",
    "\n",
    "> Average amount of money spent by the user during the themed event period\n",
    "\n",
    "> Average amount of money spent by the user during the themed event period relative to the previous month + next month (reflexive metric)\n",
    "\n",
    "> Average amount of time spent in the app\n",
    "\n",
    "### Additional metrics (metrics for scenario 1 are also relevant) for scenario 2 (for each unsuccessful attempt to complete a level, the player SHOULD roll back a few levels)\n",
    "\n",
    "> Average number of rollbacks per user \n",
    "\n",
    "> Average number of rollbacks per user by level\n",
    "\n",
    "> Metric showing how many times the levels had to be retried due to a rollback (summing up the levels retried after the rollback - total number of levels)\n",
    "\n",
    "### Метрики для сценария №1 (при каждой неудачной попытке выполнения уровня игрок НЕ БУДЕТ откатываться на несколько уровней назад)\n",
    "\n",
    "> Среднее количество пройденных уровней пользователем за событие\n",
    "\n",
    "> Среднее время прохождения каждого уровня за событие\n",
    "\n",
    "> Среднее время прохождения всех уровней за событие (для тех, кто прошел все уровни)\n",
    "\n",
    "> Показатель оттока для каждого уровня (какие уровни были особенно тяжелы для пользователей)\n",
    "\n",
    "> Среднее количество полученных уникальных предметов пользователем по разным категориям (сад, персонажи, доп. монеты, бонусы)\n",
    "\n",
    "> Количество новых уникальных пользователей во время проведения события\n",
    "\n",
    "> Показатель возврата пользователей в приложение на время тематического события\n",
    "\n",
    "> Среднее количество потраченных денег пользователем в период тематического события\n",
    "\n",
    "> Среднее количество потраченных денег пользователем в период тематического события относительно предыдущего месяца + следующего месяца (рефлексивная метрика)\n",
    "\n",
    "> Среднее количество проведенного времени в приложении\n",
    "\n",
    "### Дополнительные метрики (метрики для сценария №1 также релевантны) для сценария №2 (при каждой неудачной попытке выполнения уровня игрок БУДЕТ откатываться на несколько уровней назад)\n",
    "\n",
    "> Среднее количество откатов на пользователя \n",
    "\n",
    "> Среднее количество откатов на пользователя по уровням\n",
    "\n",
    "> Метрика, показывающая сколько раз пришлось дополнительно пройти уровни из-за отката (суммируются заново пройденные уровни после отката - общее количество уровней)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
